1.  31002 (model 2)
        Model with attention in the discriminator and a sigmoid cross entropy over all
        python train.py --e_layers=3 --batch_size=100 --caption_vector_length=100 --learning_rate=0.001 --epochs=1000 --save_every=50 --e_size=128
	python generate_images.py --model_path=Data/Models --e_layers=3 --caption_vector_length=100 --e_size=128 --n_images=100

2. 31002 (model 3)
	Model with attention in the generator
	Attn added to the last but one conv layer
	Results very bad
	python train.py --e_layers=4 --batch_size=64 --caption_vector_length=128 --learning_rate=0.001 --epochs=600 --save_every=30 --e_size=512 &
